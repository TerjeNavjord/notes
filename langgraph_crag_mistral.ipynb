{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19969669-b47f-47f3-b6d4-f7b155434840",
      "metadata": {
        "id": "19969669-b47f-47f3-b6d4-f7b155434840"
      },
      "outputs": [],
      "source": [
        "! pip install --quiet langchain_community tiktoken langchain-openai langchainhub chromadb langchain langgraph tavily-python langchain-mistralai gpt4all llama-cpp-python"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92ddc4f4-f7bf-4e0e-b5a5-5abd8a008b21",
      "metadata": {
        "id": "92ddc4f4-f7bf-4e0e-b5a5-5abd8a008b21"
      },
      "source": [
        "# Corrective RAG\n",
        "\n",
        "Self-reflection can enhance RAG, enabling correction of poor quality retrieval or generations.\n",
        "\n",
        "Several recent papers focus on this theme, but implementing the ideas can be tricky.\n",
        "\n",
        "Here we show how to implement self-reflective RAG using `Mistral` and `LangGraph`.\n",
        "\n",
        "We'll focus on ideas from one paper, `Corrective RAG (CRAG)` [here](https://arxiv.org/pdf/2401.15884.pdf).\n",
        "\n",
        "![Screenshot 2024-02-07 at 1.21.51 PM.png](attachment:a65940f9-5c51-4d7c-9ca1-ae576e4bb51a.png)\n",
        "\n",
        "## Setup\n",
        "\n",
        "### Using APIs\n",
        "\n",
        "* Set `MISTRAL_API_KEY` and set up Subscription to activate it.\n",
        "* Set `TAVILY_API_KEY` to enable web search [here](https://app.tavily.com/sign-in).\n",
        "\n",
        "### Using CoLab\n",
        "\n",
        "* [Here](https://colab.research.google.com/drive/1U5OcwWjoXZSud30q4XOk1UlIJNjaD3kX?usp=sharing) is a link to a CoLab for this notebook.\n",
        "\n",
        "### Running Locally\n",
        "\n",
        "#### Embeddings\n",
        "\n",
        "There are several options for local embeddings.\n",
        "\n",
        "(1) You can use `GPT4AllEmbeddings()` from Nomic.\n",
        "\n",
        "(2) You can also use Nomic's recently released [v1](https://blog.nomic.ai/posts/nomic-embed-text-v1) and [v1.5](https://blog.nomic.ai/posts/nomic-embed-matryoshka) embeddings.\n",
        "\n",
        "For these, simply:\n",
        "\n",
        "Clone [`llama.cpp`](https://github.com/ggerganov/llama.cpp):\n",
        "\n",
        "```\n",
        "git clone https://github.com/ggerganov/llama.cpp\n",
        "```\n",
        "\n",
        "Download GGUF weights for Nomic's embedding model(s), allowing them to be run locally:\n",
        "\n",
        "* https://huggingface.co/nomic-ai/nomic-embed-text-v1-GGUF\n",
        "* https://huggingface.co/nomic-ai/nomic-embed-text-v1.5-GGUF\n",
        "\n",
        "Add to `llama.cpp/model` directory.\n",
        "\n",
        "Build llama.cpp:\n",
        "```\n",
        "cd llama.cpp\n",
        "make\n",
        "```\n",
        "\n",
        "### LLM\n",
        "\n",
        "(1) Download [Ollama app](https://ollama.ai/).\n",
        "\n",
        "(2) Download a `Mistral` model from various Mistral versions [here](https://ollama.ai/library/mistral) and Mixtral versions [here](https://ollama.ai/library/mixtral) available.\n",
        "```\n",
        "ollama pull mistral:instruct\n",
        "```\n",
        "\n",
        "### Tracing\n",
        "\n",
        "* Optionally, use [LangSmith](https://docs.smith.langchain.com/) for tracing (shown at bottom) by setting:\n",
        "\n",
        "```\n",
        "export LANGCHAIN_TRACING_V2=true\n",
        "export LANGCHAIN_ENDPOINT=https://api.smith.langchain.com\n",
        "export LANGCHAIN_API_KEY=<your-api-key>\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "abc064ab-7de1-4d03-a987-cd3078438d61",
      "metadata": {
        "id": "abc064ab-7de1-4d03-a987-cd3078438d61"
      },
      "outputs": [],
      "source": [
        "# Check API keys\n",
        "import os\n",
        "mistral_api_key = os.environ.get(\"MISTRAL_API_KEY\")\n",
        "# get tavily api key first on https://tavily.com and set api key\n",
        "tavily_api_key = os.environ.get(\"TAVILY_API_KEY\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f644869-436e-4bf6-a267-b2465c7b5aef",
      "metadata": {
        "id": "9f644869-436e-4bf6-a267-b2465c7b5aef"
      },
      "outputs": [],
      "source": [
        "# Flags for running locally\n",
        "\n",
        "run_local = \"Yes\"\n",
        "local_llm = \"mistral:instruct\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e2b6eed-3b3f-44b5-a34a-4ade1e94caf0",
      "metadata": {
        "id": "6e2b6eed-3b3f-44b5-a34a-4ade1e94caf0"
      },
      "source": [
        "## Indexing\n",
        "\n",
        "First, let's index some blog posts on knowldege graphs.\n",
        "\n",
        "We can use [Mistral embeddings](https://python.langchain.com/docs/integrations/text_embedding/mistralai).\n",
        "\n",
        "For local, we can use [GPT4All](https://python.langchain.com/docs/integrations/text_embedding/gpt4all), which is a CPU optimized SBERT model [here](https://docs.gpt4all.io/gpt4all_python_embedding.html).\n",
        "\n",
        "We'll use a local vectorstore, [Chroma](https://python.langchain.com/docs/integrations/vectorstores/chroma)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "254ae533-79e0-42f4-b200-1ec9160e1d3d",
      "metadata": {
        "id": "254ae533-79e0-42f4-b200-1ec9160e1d3d",
        "outputId": "7e0ee377-9b13-4cff-92f9-9f4d5ac15c6b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "bert_load_from_file: gguf version     = 2\n",
            "bert_load_from_file: gguf alignment   = 32\n",
            "bert_load_from_file: gguf data offset = 695552\n",
            "bert_load_from_file: model name           = BERT\n",
            "bert_load_from_file: model architecture   = bert\n",
            "bert_load_from_file: model file type      = 1\n",
            "bert_load_from_file: bert tokenizer vocab = 30522\n"
          ]
        }
      ],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_mistralai import MistralAIEmbeddings\n",
        "from langchain_community.embeddings import GPT4AllEmbeddings\n",
        "from langchain_community.embeddings import LlamaCppEmbeddings\n",
        "\n",
        "# Load\n",
        "urls = [\n",
        "    \"https://blog.diffbot.com/let-ai-google-that-for-you/\",\n",
        "    \"https://blog.diffbot.com/generating-company-recommendations-using-large-language-models-and-knowledge-graphs/\",\n",
        "    \"https://blog.diffbot.com/grounded-natural-language-generation-with-knowledge-graphs/\",\n",
        "]\n",
        "\n",
        "docs = [WebBaseLoader(url).load() for url in urls]\n",
        "docs_list = [item for sublist in docs for item in sublist]\n",
        "\n",
        "# Split\n",
        "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "    chunk_size=500, chunk_overlap=100\n",
        ") # there are other ways to find the optimal chunk size\n",
        "all_splits = text_splitter.split_documents(docs_list)\n",
        "\n",
        "# Embed and index\n",
        "if run_local == \"Yes\":\n",
        "    # GPT4All\n",
        "    embedding = GPT4AllEmbeddings()\n",
        "    # Nomic v1 or v1.5\n",
        "    # embd_model_path = \"/Users/rlm/Desktop/Code/llama.cpp/models/nomic-embd/nomic-embed-text-v1.Q4_K_S.gguf\"\n",
        "    # embedding = LlamaCppEmbeddings(model_path=embd_model_path, n_batch=512)\n",
        "else:\n",
        "    embedding = MistralAIEmbeddings(mistral_api_key=mistral_api_key)\n",
        "\n",
        "# Index\n",
        "vectorstore = Chroma.from_documents(\n",
        "    documents=all_splits,\n",
        "    collection_name=\"rag-chroma\",\n",
        "    embedding=embedding,\n",
        ")\n",
        "retriever = vectorstore.as_retriever()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe7fd10a-f64a-48de-a116-6d5890def1af",
      "metadata": {
        "id": "fe7fd10a-f64a-48de-a116-6d5890def1af"
      },
      "source": [
        "## Corrective RAG\n",
        "\n",
        "Let's implement self-reflective RAG with some ideas from the CRAG (Corrective RAG) [paper](https://arxiv.org/pdf/2401.15884.pdf):\n",
        "\n",
        "* Grade documents for relevance relative to the question. (can be customized)\n",
        "* If any are irrelevant, then we will supplement the context used for generation with web search.\n",
        "* For web search, we will re-phrase the question and use Tavily API.\n",
        "* We will then pass retrieved documents and web results to an LLM for final answer generation.\n",
        "\n",
        "Here is a schematic of our graph in more detail:\n",
        "\n",
        "![crag.png](attachment:a2fac558-b18e-4610-bfa7-0d40c92e0ede.png)\n",
        "\n",
        "We will implement this using [LangGraph](https://python.langchain.com/docs/langgraph):\n",
        "\n",
        "* See video [here](https://www.youtube.com/watch?ref=blog.langchain.dev&v=pbAd8O1Lvm4&feature=youtu.be)\n",
        "* See blog post [here](https://blog.langchain.dev/agentic-rag-with-langgraph/)\n",
        "\n",
        "---\n",
        "\n",
        "### State\n",
        "\n",
        "Every node in our graph will modify `state`, which is dict that contains values (`question`, `documents`, etc) relevant to RAG."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10028794-2fbc-43f9-aa4c-7fe3abd69c1e",
      "metadata": {
        "id": "10028794-2fbc-43f9-aa4c-7fe3abd69c1e"
      },
      "outputs": [],
      "source": [
        "from typing import Annotated, Dict, TypedDict\n",
        "\n",
        "from langchain_core.messages import BaseMessage\n",
        "\n",
        "\n",
        "class GraphState(TypedDict):\n",
        "    \"\"\"\n",
        "    Represents the state of our graph.\n",
        "\n",
        "    Attributes:\n",
        "        keys: A dictionary where each key is a string.\n",
        "    \"\"\"\n",
        "\n",
        "    keys: Dict[str, any]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0081ff31-4a91-4dbc-9977-8bcdc7dd0aeb",
      "metadata": {
        "id": "0081ff31-4a91-4dbc-9977-8bcdc7dd0aeb"
      },
      "source": [
        "### Nodes and Edges\n",
        "\n",
        "Every node in the graph we laid out above is a function.\n",
        "\n",
        "Each node will modify the state in some way.\n",
        "\n",
        "Each edge will choose which node to call next."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "447d1333-082d-479a-a6fa-0ac0df78bb9d",
      "metadata": {
        "id": "447d1333-082d-479a-a6fa-0ac0df78bb9d"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import operator\n",
        "from typing import Annotated, Sequence, TypedDict\n",
        "\n",
        "from langchain import hub\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.schema import Document\n",
        "from langchain_community.chat_models import ChatOllama\n",
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_mistralai.chat_models import ChatMistralAI\n",
        "\n",
        "### Nodes ###\n",
        "\n",
        "\n",
        "def retrieve(state):\n",
        "    \"\"\"\n",
        "    Retrieve documents\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        state (dict): New key added to state, documents, that contains retrieved documents\n",
        "    \"\"\"\n",
        "    print(\"---RETRIEVE---\")\n",
        "    state_dict = state[\"keys\"]\n",
        "    question = state_dict[\"question\"]\n",
        "    local = state_dict[\"local\"]\n",
        "    documents = retriever.get_relevant_documents(question)\n",
        "    return {\"keys\": {\"documents\": documents, \"local\": local, \"question\": question}}\n",
        "\n",
        "\n",
        "def grade_documents(state):\n",
        "    \"\"\"\n",
        "    Determines whether the retrieved documents are relevant to the question.\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        state (dict): Updates documents key with relevant documents\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"---CHECK RELEVANCE---\")\n",
        "    state_dict = state[\"keys\"]\n",
        "    question = state_dict[\"question\"]\n",
        "    documents = state_dict[\"documents\"]\n",
        "    local = state_dict[\"local\"]\n",
        "\n",
        "    # LLM\n",
        "    if local == \"Yes\":\n",
        "        llm = ChatOllama(model=local_llm, format=\"json\", temperature=0)\n",
        "    else:\n",
        "        llm = ChatMistralAI(\n",
        "            mistral_api_key=mistral_api_key, temperature=0, model=\"mistral-medium\"\n",
        "        )\n",
        "\n",
        "    prompt = PromptTemplate(\n",
        "        template=\"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n\n",
        "        Here is the retrieved document: \\n\\n {context} \\n\\n\n",
        "        Here is the user question: {question} \\n\n",
        "        If the document contains keywords related to the user question, grade it as relevant. \\n\n",
        "        It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n",
        "        Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question. \\n\n",
        "        Provide the binary score as a JSON with a single key 'score' and no premable or explaination.\"\"\",\n",
        "        input_variables=[\"question\", \"context\"],\n",
        "    )\n",
        "\n",
        "    chain = prompt | llm | JsonOutputParser()\n",
        "\n",
        "    # Score\n",
        "    filtered_docs = []\n",
        "    search = \"No\"  # Default do not opt for web search to supplement retrieval\n",
        "    for d in documents:\n",
        "        score = chain.invoke(\n",
        "            {\n",
        "                \"question\": question,\n",
        "                \"context\": d.page_content,\n",
        "            }\n",
        "        )\n",
        "        grade = score[\"score\"]\n",
        "        if grade == \"yes\":\n",
        "            print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
        "            filtered_docs.append(d)\n",
        "        else:\n",
        "            print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
        "            search = \"Yes\"  # Perform web search\n",
        "            continue\n",
        "\n",
        "    return {\n",
        "        \"keys\": {\n",
        "            \"documents\": filtered_docs,\n",
        "            \"question\": question,\n",
        "            \"local\": local,\n",
        "            \"run_web_search\": search,\n",
        "        }\n",
        "    }\n",
        "\n",
        "\n",
        "def transform_query(state): # can implement DSPy in furture cases\n",
        "    \"\"\"\n",
        "    Transform the query to produce a better question.\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        state (dict): Updates question key with a re-phrased question\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"---TRANSFORM QUERY---\")\n",
        "    state_dict = state[\"keys\"]\n",
        "    question = state_dict[\"question\"]\n",
        "    documents = state_dict[\"documents\"]\n",
        "    local = state_dict[\"local\"]\n",
        "\n",
        "    # Create a prompt template with format instructions and the query\n",
        "    prompt = PromptTemplate(\n",
        "        template=\"\"\"You are generating questions that is well optimized for retrieval. \\n\n",
        "        Look at the input and try to reason about the underlying sematic intent / meaning. \\n\n",
        "        Here is the initial question:\n",
        "        \\n ------- \\n\n",
        "        {question}\n",
        "        \\n ------- \\n\n",
        "        Provide an improved question without any premable, only respond with the updated question: \"\"\",\n",
        "        input_variables=[\"question\"],\n",
        "    )\n",
        "\n",
        "    # Grader\n",
        "    # LLM\n",
        "    if local == \"Yes\":\n",
        "        llm = ChatOllama(model=local_llm, temperature=0)\n",
        "    else:\n",
        "        llm = ChatMistralAI(\n",
        "            mistral_api_key=mistral_api_key, temperature=0, model=\"mistral-medium\"\n",
        "        )\n",
        "\n",
        "    # Prompt\n",
        "    chain = prompt | llm | StrOutputParser()\n",
        "    better_question = chain.invoke({\"question\": question})\n",
        "\n",
        "    return {\n",
        "        \"keys\": {\"documents\": documents, \"question\": better_question, \"local\": local}\n",
        "    }\n",
        "\n",
        "\n",
        "def web_search(state): # for TavilySearch\n",
        "    \"\"\"\n",
        "    Web search based on the re-phrased question using Tavily API.\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        state (dict): Web results appended to documents.\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"---WEB SEARCH---\")\n",
        "    state_dict = state[\"keys\"]\n",
        "    question = state_dict[\"question\"]\n",
        "    documents = state_dict[\"documents\"]\n",
        "    local = state_dict[\"local\"]\n",
        "\n",
        "    tool = TavilySearchResults()\n",
        "    docs = tool.invoke({\"query\": question})\n",
        "    web_results = \"\\n\".join([d[\"content\"] for d in docs])\n",
        "    web_results = Document(page_content=web_results)\n",
        "    documents.append(web_results)\n",
        "\n",
        "    return {\"keys\": {\"documents\": documents, \"local\": local, \"question\": question}}\n",
        "\n",
        "\n",
        "### Edges\n",
        "\n",
        "\n",
        "def decide_to_generate(state):\n",
        "    \"\"\"\n",
        "    Determines whether to generate an answer or re-generate a question for web search.\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current state of the agent, including all keys.\n",
        "\n",
        "    Returns:\n",
        "        str: Next node to call\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"---DECIDE TO GENERATE---\")\n",
        "    state_dict = state[\"keys\"]\n",
        "    question = state_dict[\"question\"]\n",
        "    filtered_documents = state_dict[\"documents\"]\n",
        "    search = state_dict[\"run_web_search\"]\n",
        "\n",
        "    if search == \"Yes\":\n",
        "        # All documents have been filtered check_relevance\n",
        "        # We will re-generate a new query\n",
        "        print(\"---DECISION: TRANSFORM QUERY and RUN WEB SEARCH---\")\n",
        "        return \"transform_query\"\n",
        "    else:\n",
        "        # We have relevant documents, so generate answer\n",
        "        print(\"---DECISION: GENERATE---\")\n",
        "        return \"generate\"\n",
        "\n",
        "\n",
        "\n",
        "def generate(state):\n",
        "    \"\"\"\n",
        "    Generate answer\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        state (dict): New key added to state, generation, that contains generation\n",
        "    \"\"\"\n",
        "    print(\"---GENERATE---\")\n",
        "    state_dict = state[\"keys\"]\n",
        "    question = state_dict[\"question\"]\n",
        "    documents = state_dict[\"documents\"]\n",
        "    local = state_dict[\"local\"]\n",
        "\n",
        "    # Prompt\n",
        "    prompt = hub.pull(\"rlm/rag-prompt\")\n",
        "\n",
        "    # LLM\n",
        "    if local == \"Yes\":\n",
        "        llm = ChatOllama(model=local_llm, temperature=0)\n",
        "    else:\n",
        "        llm = ChatMistralAI(\n",
        "            model=\"mistral-medium\", temperature=0, mistral_api_key=mistral_api_key\n",
        "        )\n",
        "\n",
        "    # Post-processing\n",
        "    def format_docs(docs):\n",
        "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "    # Chain\n",
        "    rag_chain = prompt | llm | StrOutputParser()\n",
        "\n",
        "    # Run\n",
        "    generation = rag_chain.invoke({\"context\": documents, \"question\": question})\n",
        "    return {\n",
        "        \"keys\": {\"documents\": documents, \"question\": question, \"generation\": generation}\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6096626d-dfa5-48e0-8a24-3747b298bc67",
      "metadata": {
        "id": "6096626d-dfa5-48e0-8a24-3747b298bc67"
      },
      "source": [
        "## Build Graph\n",
        "\n",
        "This just follows the flow we outlined in the figure above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a63776c-f9cd-46ce-b8cf-95c066dc5b06",
      "metadata": {
        "id": "0a63776c-f9cd-46ce-b8cf-95c066dc5b06"
      },
      "outputs": [],
      "source": [
        "import pprint\n",
        "\n",
        "from langgraph.graph import END, StateGraph\n",
        "\n",
        "workflow = StateGraph(GraphState)\n",
        "\n",
        "# Define the nodes\n",
        "workflow.add_node(\"retrieve\", retrieve)  # retrieve\n",
        "workflow.add_node(\"grade_documents\", grade_documents)  # grade documents\n",
        "workflow.add_node(\"generate\", generate)  # generatae\n",
        "workflow.add_node(\"transform_query\", transform_query)  # transform_query\n",
        "workflow.add_node(\"web_search\", web_search)  # web search\n",
        "\n",
        "# Build graph\n",
        "workflow.set_entry_point(\"retrieve\")\n",
        "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
        "workflow.add_conditional_edges(\n",
        "    \"grade_documents\",\n",
        "    decide_to_generate,\n",
        "    {\n",
        "        \"transform_query\": \"transform_query\",\n",
        "        \"generate\": \"generate\",\n",
        "    },\n",
        ")\n",
        "workflow.add_edge(\"transform_query\", \"web_search\")\n",
        "workflow.add_edge(\"web_search\", \"generate\")\n",
        "workflow.add_edge(\"generate\", END)\n",
        "\n",
        "# Compile\n",
        "app = workflow.compile()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac0a868a-f0f9-4aa9-b955-a78da2359af8",
      "metadata": {
        "id": "ac0a868a-f0f9-4aa9-b955-a78da2359af8"
      },
      "source": [
        "## Run\n",
        "\n",
        "`Mistral API -`\n",
        "\n",
        "Trace for below run: https://smith.langchain.com/public/0a5cbc97-a2f6-4697-856c-90a6302fd13e/r"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ab1d8df-a74e-4b48-a30b-e39bbfd5925a",
      "metadata": {
        "id": "3ab1d8df-a74e-4b48-a30b-e39bbfd5925a",
        "outputId": "1064ac50-2fc5-48dd-c4aa-8147c630e886"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---RETRIEVE---\n",
            "\"Node 'retrieve':\"\n",
            "'\\n---\\n'\n",
            "---CHECK RELEVANCE---\n",
            "---GRADE: DOCUMENT RELEVANT---\n",
            "---GRADE: DOCUMENT RELEVANT---\n",
            "---GRADE: DOCUMENT RELEVANT---\n",
            "---GRADE: DOCUMENT RELEVANT---\n",
            "\"Node 'grade_documents':\"\n",
            "'\\n---\\n'\n",
            "---DECIDE TO GENERATE---\n",
            "---DECISION: GENERATE---\n",
            "---GENERATE---\n",
            "\"Node 'generate':\"\n",
            "'\\n---\\n'\n",
            "\"Node '__end__':\"\n",
            "'\\n---\\n'\n",
            "(' Knowledge graph-based systems improve factual accuracy compared to those '\n",
            " 'without knowledge graphs because they use precise and structured data '\n",
            " 'representations, unlike Large Language Models (LLMs) that are optimized for '\n",
            " 'creativity and abstract representation of information. Knowledge Graphs '\n",
            " 'store facts at web-scale losslessly with provenance, enabling hybrid systems '\n",
            " \"that combine the LLM's strong abilities in transforming text and the \"\n",
            " 'deterministic execution against a precise intent from knowledge graphs. This '\n",
            " 'results in accurate and authoritative information without room for '\n",
            " 'hallucinations or corruption during retrieval. (Reference: Documents 1, 2, '\n",
            " 'and 3)')\n"
          ]
        }
      ],
      "source": [
        "# Run\n",
        "inputs = {\n",
        "    \"keys\": {\n",
        "        \"question\": \"Why can knowledge graph-based systems improve factual accuracy compared to those without knowlegde graphs?\",\n",
        "        \"local\": run_local,\n",
        "    }\n",
        "}\n",
        "for output in app.stream(inputs):\n",
        "    for key, value in output.items():\n",
        "        # Node\n",
        "        pprint.pprint(f\"Node '{key}':\")\n",
        "        # Optional: print full state at each node\n",
        "        # pprint.pprint(value[\"keys\"], indent=2, width=80, depth=None)\n",
        "    pprint.pprint(\"\\n---\\n\")\n",
        "\n",
        "# Final generation\n",
        "pprint.pprint(value[\"keys\"][\"generation\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03ee2be9-2368-46ea-9edd-dc064a7c7c96",
      "metadata": {
        "id": "03ee2be9-2368-46ea-9edd-dc064a7c7c96"
      },
      "source": [
        "`Local (Ollama) -`\n",
        "\n",
        "Trace for blow run: https://smith.langchain.com/public/3b23a1d4-720a-4b26-8f34-70d2f20f8832/r"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa83a8d4-f97f-4802-8e86-459e153d10a2",
      "metadata": {
        "id": "aa83a8d4-f97f-4802-8e86-459e153d10a2",
        "outputId": "fc6cb752-8a11-4328-de7f-9dbabc1e5e54"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---RETRIEVE---\n",
            "\"Node 'retrieve':\"\n",
            "'\\n---\\n'\n",
            "---CHECK RELEVANCE---\n",
            "---GRADE: DOCUMENT RELEVANT---\n",
            "---GRADE: DOCUMENT RELEVANT---\n",
            "---GRADE: DOCUMENT RELEVANT---\n",
            "---GRADE: DOCUMENT RELEVANT---\n",
            "\"Node 'grade_documents':\"\n",
            "'\\n---\\n'\n",
            "---DECIDE TO GENERATE---\n",
            "---DECISION: GENERATE---\n",
            "---GENERATE---\n",
            "\"Node 'generate':\"\n",
            "'\\n---\\n'\n",
            "\"Node '__end__':\"\n",
            "'\\n---\\n'\n",
            "(' If LLM-based systems are not grounded with knowledge graphs, they may '\n",
            " 'generate inaccurate or hallucinated facts without any provenance or factual '\n",
            " 'basis. This can lead to misinformation being presented to users, potentially '\n",
            " 'causing confusion or incorrect decisions. Additionally, the lack of a '\n",
            " \"knowledge graph may hinder the system's ability to enhance recommendations \"\n",
            " 'based on up-to-date and relevant information.')\n"
          ]
        }
      ],
      "source": [
        "# Run\n",
        "inputs = {\n",
        "    \"keys\": {\n",
        "        \"question\": \"What could be the consequences if LLM-based systems are not grounded with knowlegde graphs?\",\n",
        "        \"local\": run_local,\n",
        "    }\n",
        "}\n",
        "for output in app.stream(inputs):\n",
        "    for key, value in output.items():\n",
        "        # Node\n",
        "        pprint.pprint(f\"Node '{key}':\")\n",
        "        # Optional: print full state at each node\n",
        "        # pprint.pprint(value[\"keys\"], indent=2, width=80, depth=None)\n",
        "    pprint.pprint(\"\\n---\\n\")\n",
        "\n",
        "# Final generation\n",
        "pprint.pprint(value[\"keys\"][\"generation\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5855d8c-0cc5-4c12-860d-e776567fe80c",
      "metadata": {
        "id": "b5855d8c-0cc5-4c12-860d-e776567fe80c",
        "outputId": "d1a94020-3b0a-458c-c337-51d9688971be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---RETRIEVE---\n",
            "\"Node 'retrieve':\"\n",
            "'\\n---\\n'\n",
            "---CHECK RELEVANCE---\n",
            "---GRADE: DOCUMENT RELEVANT---\n",
            "---GRADE: DOCUMENT NOT RELEVANT---\n",
            "---GRADE: DOCUMENT NOT RELEVANT---\n",
            "---GRADE: DOCUMENT RELEVANT---\n",
            "\"Node 'grade_documents':\"\n",
            "'\\n---\\n'\n",
            "---DECIDE TO GENERATE---\n",
            "---DECISION: TRANSFORM QUERY and RUN WEB SEARCH---\n",
            "---TRANSFORM QUERY---\n",
            "\"Node 'transform_query':\"\n",
            "'\\n---\\n'\n",
            "---WEB SEARCH---\n",
            "\"Node 'web_search':\"\n",
            "'\\n---\\n'\n",
            "---GENERATE---\n",
            "\"Node 'generate':\"\n",
            "'\\n---\\n'\n",
            "\"Node '__end__':\"\n",
            "'\\n---\\n'\n",
            "(' Jason Pargin encountered difficulty finding accurate information on how to '\n",
            " 'set a wifi network as primary on iOS through regular search engines. He was '\n",
            " 'instead led to outdated articles and ads. However, AI search tools that '\n",
            " 'filter through the crud of search results exist and could have provided him '\n",
            " 'with more precise answers. Additionally, Knowledge Graphs, which store facts '\n",
            " 'at web-scale losslessly and with provenance, can guide language generation '\n",
            " 'for more accurate results.')\n"
          ]
        }
      ],
      "source": [
        "# Run\n",
        "inputs = {\n",
        "    \"keys\": {\n",
        "        \"question\": \"What challenge did Jason Pargin face?\",\n",
        "        \"local\": run_local,\n",
        "    }\n",
        "}\n",
        "for output in app.stream(inputs):\n",
        "    for key, value in output.items():\n",
        "        # Node\n",
        "        pprint.pprint(f\"Node '{key}':\")\n",
        "        # Optional: print full state at each node\n",
        "        # pprint.pprint(value[\"keys\"], indent=2, width=80, depth=None)\n",
        "    pprint.pprint(\"\\n---\\n\")\n",
        "\n",
        "# Final generation\n",
        "pprint.pprint(value[\"keys\"][\"generation\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56a21bb2-fe80-4357-8ded-b0f199ee407f",
      "metadata": {
        "id": "56a21bb2-fe80-4357-8ded-b0f199ee407f"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}