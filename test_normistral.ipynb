{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TerjeNavjord/notes/blob/main/test_normistral.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example of using NorMistral on a GPU with low VRAM"
      ],
      "metadata": {
        "id": "YgKcuxQKprt9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialize the environment"
      ],
      "metadata": {
        "id": "1Y07jLUIp6Tn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "H48TM4h2Y-AR"
      },
      "outputs": [],
      "source": [
        "!pip install bitsandbytes\n",
        "!pip install accelerate\n",
        "\n",
        "import torch\n",
        "from google.colab import output\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "output.clear()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the model\n",
        "\n",
        "The model will be quantized into 8 bits, it trades off performance for less memory usage."
      ],
      "metadata": {
        "id": "qAEm5alCp2Q5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'norallm/normistral-7b-warm' # @param [\"norallm/normistral-7b-warm\", \"norallm/normistral-7b-scratch\", \"norallm/norbloom-7b-scratch\"]"
      ],
      "metadata": {
        "cellView": "form",
        "id": "-C2u4Vb7pcGP"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    model_name\n",
        ")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map='auto',\n",
        "    low_cpu_mem_usage=True,\n",
        "    load_in_8bit=True,\n",
        "    torch_dtype=torch.bfloat16\n",
        ")\n",
        "output.clear()"
      ],
      "metadata": {
        "id": "SFGF5LSbZBil"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define a prompt for zero-shot machine translation"
      ],
      "metadata": {
        "id": "MDR1g6YDqZfN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Zero-shot prompt template\n",
        "prompt = \"\"\"{source_language}: {source_text}\n",
        "{target_language}:\"\"\"\n",
        "\n",
        "# A function that will take care of generating the output\n",
        "@torch.no_grad()\n",
        "def generate(input_dict):\n",
        "    text = prompt.format(**input_dict)\n",
        "    input_ids = tokenizer(text, return_tensors='pt').input_ids.cuda()\n",
        "    prediction = model.generate(\n",
        "        input_ids,\n",
        "        max_new_tokens=256,\n",
        "        do_sample=False,\n",
        "        eos_token_id=tokenizer('\\n').input_ids\n",
        "    )\n",
        "    decoded_prediction = tokenizer.decode(prediction[0, input_ids.size(1):]).strip()\n",
        "    output.clear()\n",
        "\n",
        "    return decoded_prediction"
      ],
      "metadata": {
        "id": "-G6IK4JBayiz"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Translate!"
      ],
      "metadata": {
        "id": "8F7hGAMcqqBn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "source_text = \"In contrast, as mentioned earlier, in the civil law system, legal language is more formal and precise, while in the common law system, legal language is more flexible and adaptable. Court System: In Roman law, the court system was organized into a hierarchy of courts, with the emperor as the ultimate authority. The courts were responsible for interpreting and applying the legal code, and their decisions were subject to review by higher courts. In contrast, as mentioned earlier, in the civil law system, the court system is typically organized into a hierarchy of courts, while in the common law system, the court system is more complex, with both federal and state courts. Jury Trials: In Roman law, jury trials were not a feature of the legal system. Judges were responsible for determining both questions of law and questions of fact. In contrast, as mentioned earlier, in the civil law system, jury trials are rare, while in the common law system, jury trials are a fundamental feature of the legal system. Precedent: In Roman law, prior decisions were not binding on future courts. However, the writings of Roman jurists and the decisions of the emperor were highly influential and often followed by later courts. In contrast, as mentioned earlier, in the civil law system, prior decisions are not binding on future courts, while in the common law system, prior decisions are binding on future courts through the doctrine of stare decisis. In summary, Roman law is characterized by a comprehensive legal code, an active role for judges, formal legal language, a hierarchical court system, no jury trials, and influential but not binding prior decisions. Both the civil law and common law systems have roots in Roman law, but they have evolved differently over time, with the civil law system more closely resembling Roman law in some respects, such as its comprehensive legal code and formal legal language.\" # @param {type:\"string\"}\n",
        "source_language = \"Engelsk\" # @param [\"Engelsk\", \"Bokmål\", \"Nynorsk\"]\n",
        "target_language = \"Nynorsk\" # @param [\"Engelsk\", \"Bokmål\", \"Nynorsk\"]"
      ],
      "metadata": {
        "id": "NsMfsv6gqvD4"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_text = generate({\n",
        "    \"source_text\": source_text,\n",
        "    \"source_language\": source_language,\n",
        "    \"target_language\": target_language\n",
        "})\n",
        "\n",
        "print(output_text)"
      ],
      "metadata": {
        "id": "aKFKAU3Da1vS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7efe8081-86d8-4033-a3a1-290431a129d6"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}